A menu was created so that you can run the crawling, save the graph, make the loglog plot, run the page rank, and upload a file. For the web crawling part of the assignment I decided to use Scrapy after speaking with the professor about possible ways of implementing this crawling. I specifically used "CrawlSpider" from scrappy and created a class with the desired allowed domains and start_urls to run the web crawler on. The urls were parsed from a sample txt file that I created using the sample given in the assignment. To make sure the correct amount of nodes and correct links were retreived a added if statements and for loops to ensure that the relationships between urls and unique urls themselves were stored in a dictionary and a set. Then I created the graph using networkx. The loglog plot was made using the matplotlib built in loglog plot function. Once the graph is created you can then save it, run it through the loglog plot creation or perform the pagerank method on it. For PageRank I used the built in networkx library to calculate the page rank and ask the user to specify the number of iterations. Then a graph is created for the desired value ranges and havign the node sizes being relative to their rank and color as well. The color darkness is used as a scale between 0 and 1 of where it falls in the page rank. A bar was created to display the difference in shades from 0 to 1. Finally the pageranks are saved to a txt file.

CHALLENGES:
I was faced with the challenge of figuring out how Scrapy even worked for most of my time. The main issue that I was still not able to resolve was fixing the 429 error that is recieved when running iterations over 100 for web crawling. This was semi-fixed with adding a DOWNLOAD_DELAY of 0.5 however this makes the runtime of my program really long. It takes about an 50 minutes to finish doing 5000 nodes. I tried other solutions such as lowering the concurrent requests, adding an exception for getting the response code 429 and adding in multiple different user agents so that it can avoid being detected. However nothing besides just adding the download delay actually fixed the solution I believe. 

Additional Featuers:
I added the option to upload a graph to my program so that I don't have to run the crawling everytime to obtain my graph. I also added a color bar to my pagerank graph to help show the difference in color shades.